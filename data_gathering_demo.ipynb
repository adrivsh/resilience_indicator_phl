{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how Python can be used to gather and adapt data from different sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading socio-economic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the [pandas](http://pandas.pydata.org/) function librairy. Pandas is a standard python librairy that alows us to manipulate Excel-like tables (called DataFrames) with named rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:39.676232",
     "start_time": "2016-03-01T17:08:38.764811"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the excel data into a pandas DataFrame.\n",
    "We start from an Excel file that contains socio-economic data. In the future this file may for instance be populated by PSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:39.950932",
     "start_time": "2016-03-01T17:08:39.680233"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'inputs/input_data_Feb2016.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7c18c407d946>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m                         \u001b[0msheetname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Consolidated (2012)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#the Excel tab were the data is\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Province\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;31m#column to use as index\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                                \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#skips the first line of the excel file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m                                 );\n\u001b[0;32m      6\u001b[0m \u001b[0mdata_from_excel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_from_excel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#fixes the case of province names in the Excel file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheetname, header, skiprows, skip_footer, index_col, parse_cols, parse_dates, date_parser, na_values, thousands, convert_float, has_index_names, converters, engine, **kwds)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     return io._parse_excel(\n",
      "\u001b[1;32mC:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, **kwds)\u001b[0m\n\u001b[0;32m    204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'xlrd'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\xlrd\\__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'inputs/input_data_Feb2016.xlsx'"
     ]
    }
   ],
   "source": [
    "data_from_excel= pd.read_excel(\"inputs/input_data_Feb2016.xlsx\", #the name of the file\n",
    "                        sheetname=\"Consolidated (2012)\", #the Excel tab were the data is\n",
    "                               index_col=\"Province\",#column to use as index\n",
    "                               header=1, #skips the first line of the excel file\n",
    "                                );\n",
    "data_from_excel.index = data_from_excel.index.str.title() #fixes the case of province names in the Excel file\n",
    "data_from_excel.head() #shows the first few lines of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table contains more data (more columns) that what we need to run the model. In addition, the names of the coumn are human-readable, instead of correspondig to variable names in the model. Finally, Some data is missing. We solve each one of this problems in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching columns in the Excel file to variables in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### pov_head, pop, gdp_pc_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the data in the Excel file match directly data in the model. We can transform them directly using a simple dictionary, [inputs/data_source_matching.csv](inputs/data_source_matching.csv), that matches the name in the Excel file to the name in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:39.966447",
     "start_time": "2016-03-01T17:08:39.953435"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reads the CSV file that matches names in excel ot names in the model\n",
    "data_source_matching =pd.read_csv(\"inputs/data_source_matching.csv\",\n",
    "                                  index_col=\"name_in_data\",\n",
    "                                 )\n",
    "data_source_matching #displays the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.035492",
     "start_time": "2016-03-01T17:08:39.970448"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#keeps only the colomns listed in data_source_matching\n",
    "df=data_from_excel[data_source_matching.index]\n",
    "#renames those columns to their name in the model\n",
    "df=df.rename(columns=data_source_matching[\"name_in_model\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adapting the data on income and poverty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model needs income information in each province to be provided relative to the average income in the Philippines.\n",
    "Witin each province, we need the income of the poor and nonpoor households relative to the average income in the province.\n",
    "\n",
    "To compute the weighted average, we will use another standard python library, [NumPy](http://www.numpy.org/) the provides  standard mathematical functions such as log, exp, weighted average, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.042502",
     "start_time": "2016-03-01T17:08:40.039500"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.074531",
     "start_time": "2016-03-01T17:08:40.045504"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Changes the unit of GDP to thousands of pesos (technical: to reduce risk of float overflows when computing welfare)\n",
    "df[\"gdp_pc_pp\"]/=1e3\n",
    "\n",
    "#National average income \n",
    "df[\"gdp_pc_pp_nat\"] = np.average(df.dropna().gdp_pc_pp,  weights=df.dropna()[\"pop\"]) #note that we have to manually remove the lines with missing data (.dropna()) because numpy does not handle missing data\n",
    "\n",
    "#Average income of poor households (estimated from WB data on income distribution: http://iresearch.worldbank.org/PovcalNet/index.htm?2)\n",
    "wp=50\n",
    "\n",
    "#Relative income of the province and poor families in those provinces\n",
    "df[\"rel_gdp_pp\"]=df[\"gdp_pc_pp\"]/df[\"gdp_pc_pp_nat\"]\n",
    "df[\"share1\"]=wp/df[\"gdp_pc_pp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access to savings, transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other model variables do not match directly one column in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.141579",
     "start_time": "2016-03-01T17:08:40.080536"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#acess to bank accounts : we use the same value for poor and nonpoor households\n",
    "df[\"axfin_p\"]=df[\"axfin_r\"]=data_from_excel[\"%Savings Deposit 2012\"]\n",
    "\n",
    "#share of income from transfers: we use the same value for poor and nonpoor, and we sum two columns of the input data\n",
    "df[\"social_p\"]=df[\"social_r\"]=data_from_excel[[\"% Other sources of income 2012\",\"% Other receipts 2012\"]].sum(axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data on exposure, hazard, and protection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exposure (population in flood-prone areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exposure comes from a different file, for instance it could be provided by DOST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.191616",
     "start_time": "2016-03-01T17:08:40.174604"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Exposure to floods (from glofris)\n",
    "pop_exposed = pd.read_csv(\"inputs/pop_exposed.csv\",index_col=[\"NAME_1\"])\n",
    "pop_exposed.index=pop_exposed.index.str.title()\n",
    "pop_exposed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how for some provinces (Aklan, Albay) are not exposed to river flodds according to our data source. Also, the data we have here is for several return periods. The model can work either with on single return period or several return periods. The information on different exposed periods sorted in a different variable, `fa_ratios`.\n",
    "\n",
    "First we define the exposure (Fraction of people Affected) as the one corresponding to 10 yr return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.264673",
     "start_time": "2016-03-01T17:08:40.196619"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"fa\"]=pop_exposed[\"rp10_pop\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the exposure to other return period events relative to the exposure to the 10yr event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.309704",
     "start_time": "2016-03-01T17:08:40.268676"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fa_ratios =pop_exposed.div(df[\"fa\"],axis=0)\n",
    "fa_ratios.columns=[10,100]\n",
    "fa_ratios.to_csv(\"fa_ratios.csv\") \n",
    "fa_ratios.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the provinces with no exposure, we get NaN (not a number), because of the division by 0. The pandas dataframe handle missing data seamlessly.\n",
    "The method dropna() allows to drop the ines for which some data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.338723",
     "start_time": "2016-03-01T17:08:40.312705"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"In the dataset we currenty use, there are {n} provinces with information on exposure.\".format(n=len(fa_ratios.dropna().index)))\n",
    "fa_ratios.dropna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vulnerability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess asset vulnerability in each province, we use census data on roof and wall types in each province.\n",
    "We match these types to a given vulnerability with reduced vulnerability curves. Let us first open the files that matche wall and roof types to vulnerability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T15:46:10.820589",
     "start_time": "2016-03-01T15:46:10.818588"
    }
   },
   "source": [
    "#### Reduced vulnerability curves for wall and roofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.368752",
     "start_time": "2016-03-01T17:08:40.342730"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#matches roof and wall types to vulnerabilities\n",
    "roof_types_to_vuln =pd.read_csv(\"inputs/roof_types_to_vuln.csv\").squeeze().sort_values(ascending=False)\n",
    "wall_types_to_vuln =pd.read_csv(\"inputs/wall_types_to_vuln.csv\").squeeze().sort_values(ascending=False)\n",
    "\n",
    "print(\"Reduced vulnerability curve for roofs\\n\")\n",
    "print(roof_types_to_vuln)\n",
    "#print(\"\\nReduced vulnerability curve for walls\")\n",
    "#print(wall_types_to_vuln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T15:47:04.801439",
     "start_time": "2016-03-01T15:47:04.799439"
    }
   },
   "source": [
    "#### Sorting roofs according to income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for **roof** types in each province come from the excel file with socio-economic data we used at the begining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.409778",
     "start_time": "2016-03-01T17:08:40.381760"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "share =data_from_excel[roof_types_to_vuln.index]\n",
    "share.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we assume that the poorest households in  each province use the houses with lowest quality roofs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.447808",
     "start_time": "2016-03-01T17:08:40.416783"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sorts roof types according to income\n",
    "p=(share.cumsum(axis=1).add(-df[\"pov_head\"],axis=0)).clip(lower=0)\n",
    "poor=(share-p).clip(lower=0)\n",
    "rich=share-poor\n",
    "\n",
    "print(\"Type of roofs for nonpoor households:\")\n",
    "rich.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we average vulnerability accross roof types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.464824",
     "start_time": "2016-03-01T17:08:40.450810"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#averages vulnerability accross roof type\n",
    "vp_roof=((poor*roof_types_to_vuln).sum(axis=1)/df[\"pov_head\"] )\n",
    "vr_roof=(rich*roof_types_to_vuln).sum(axis=1)/(1-df[\"pov_head\"])\n",
    "\n",
    "vp_roof.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T15:47:04.801439",
     "start_time": "2016-03-01T15:47:04.799439"
    }
   },
   "source": [
    "#### Sorting walls according to income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do the same for <b>walls</b>..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.503851",
     "start_time": "2016-03-01T17:08:40.468826"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sorts wall types according to income\n",
    "share =data_from_excel[wall_types_to_vuln.keys()]\n",
    "p=(share.cumsum(axis=1).add(-df[\"pov_head\"],axis=0)).clip(lower=0)\n",
    "poor=(share-p).clip(lower=0)\n",
    "rich=share-poor\n",
    "\n",
    "#walls\n",
    "vp_wall=((poor*wall_types_to_vuln).sum(axis=1)/df[\"pov_head\"] )\n",
    "vr_wall=(rich*wall_types_to_vuln).sum(axis=1)/(1-df[\"pov_head\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and take the average value for roof and walls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.516858",
     "start_time": "2016-03-01T17:08:40.507854"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#averages value for roofs and walls\n",
    "vp = (vp_roof+vp_wall)/2\n",
    "vr = (vr_roof+vr_wall)/2\n",
    "\n",
    "#plots\n",
    "#vp.hist(), plt.xlabel(\"vp\")\n",
    "#plt.figure()\n",
    "#vr.hist(),plt.xlabel(\"vr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting the data on exposure and vulnerability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model needs the information on exposure and vulnerability within each province to be provided as an <b>average</b> and <b>a bias for poor households</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.556894",
     "start_time": "2016-03-01T17:08:40.520861"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We only have average exposure, so we assume an exposure poverty bias of 20%\n",
    "#This is a data gap that could be filled later\n",
    "pe=df[\"pe\"] = .2\n",
    "\n",
    "#Expresses vulnerability as total and bias\n",
    "ph=df[\"pov_head\"]\n",
    "fa=df[\"fa\"]\n",
    "fap=fa*(1+pe)\n",
    "far=(fa-ph*fap)/(1-ph)\n",
    "\n",
    "cp=   df[\"share1\"] *df[\"gdp_pc_pp\"]/ph\n",
    "cr=(1-df[\"share1\"])*df[\"gdp_pc_pp\"]/(1-ph)\n",
    "\n",
    "v=df[\"v\"]  = (ph*vp*cp*fap + (1-ph)*vr*cr*far)/(ph*cp*fap + (1-ph)*cr*far)\n",
    "df[\"pv\"] =  vp/df.v-1\n",
    "\n",
    "#vulnerability of diversified (shared) capital\n",
    "df[\"v_s\"]=vr\n",
    "\n",
    "# %matplotlib inline\n",
    "# vp .hist(alpha=0.5)\n",
    "# vr.hist(alpha=0.5)\n",
    "# v.hist(alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hazard (protection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We capture hazard through the protection level, given in return period. Here we use data from FLOPROS as a placeholder.\n",
    "FLOPROS uses a different spelling for some province, so we correct that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.588917",
     "start_time": "2016-03-01T17:08:40.563899"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "protection = pd.read_csv(\"inputs/protection_phl.csv\",index_col=\"province\", squeeze=True).sort_index()\n",
    "protection.index = protection.index.str.title()\n",
    "protection.rename(index={\"Cotabato\":\"North Cotabato\",\n",
    "                         'Mindoro Occidental':\"Occidental Mindoro\",\n",
    "                         'Mindoro Oriental':\"Oriental Mindoro\",}, inplace=True) #(an altenrative way would be to use and demonstrate the function replace_with_warning)\n",
    "protection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.613934",
     "start_time": "2016-03-01T17:08:40.601925"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"protection\"]=protection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually filling data gaps and informing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some data is missing and has to be added manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.645458",
     "start_time": "2016-03-01T17:08:40.635947"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#average productivity of capital\n",
    "df[\"avg_prod_k\"] = .23\n",
    "\n",
    "#Reconstruction time (an only be guessed ex-ante)\n",
    "df[\"T_rebuild_K\"] = 3\n",
    "\n",
    "# how much early warning reduces vulnerability (eg reactivity to early warnings)\n",
    "df[\"pi\"] = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other inputs are normative or policy choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.674983",
     "start_time": "2016-03-01T17:08:40.648460"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#assumption on cross-provincial risk sharing\n",
    "df[\"nat_buyout\"] = 0.3\n",
    "\n",
    "#scale up of transfers after the \n",
    "df[\"sigma_r\"]=df[\"sigma_p\"]=1/3\n",
    "\n",
    "#income elasticity\n",
    "df[\"income_elast\"] = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adds description to the variables names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add a human readable descritpion to all model variables, based on the descriptions gathered in [inputs/inputs_info.csv](inputs/inputs_info.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:40.824096",
     "start_time": "2016-03-01T17:08:40.677986"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "description = pd.read_csv(\"inputs/inputs_info.csv\", index_col=\"key\")[\"descriptor\"]\n",
    "df.ix[\"description\"]= description\n",
    "data=df.T.reset_index().set_index([\"description\",\"index\"]).T\n",
    "data.columns.names = ['description', 'variable']\n",
    "data.head().T #displays the first few provinces, transposed for ease of reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saves the table with compiled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:41.323464",
     "start_time": "2016-03-01T17:08:40.828098"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#saves orginal dataframe before adding columns with results\n",
    "data.to_excel(\"all_data_compiled.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's it, we have built an excel file with all our data!**\n",
    "To see how to use this data with the resilience model, go to [socio_economic_capacity_demo.ipynb](socio_economic_capacity_demo.ipynb)\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report missing data by province"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code builds a table reporting missing data points for each province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:41.324464",
     "start_time": "2016-03-01T22:08:38.906Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_missing_data(s):\n",
    "    which = s[s.isnull()].index.values\n",
    "    return \", \".join(which)\n",
    "\n",
    "def count_missing_data(s):\n",
    "    return s.isnull().sum()\n",
    "\n",
    "report = pd.DataFrame()\n",
    "\n",
    "report[\"nb_missing\"]=df.apply(count_missing_data,axis=1)  \n",
    "report[\"missing_data\"]=df.apply(write_missing_data,axis=1)\n",
    "\n",
    "report  = report.ix[report[\"nb_missing\"]>0,:]\n",
    "report.sort_values(by=\"nb_missing\",inplace=True)\n",
    "report.to_csv(\"missing_data_report.csv\")\n",
    "\n",
    "report.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for two provinces, we have no data on protection. Let us inspect the data on protection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T17:08:41.325468",
     "start_time": "2016-03-01T22:08:38.910Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "protection.ix[[\"Misamis Occidental\", \"Negros Oriental\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-01T15:55:15.357382",
     "start_time": "2016-03-01T15:55:15.351381"
    }
   },
   "source": [
    "In our data on protection, these two provinces have a missing value (nan). This probelm should be investigated going back to the source used for protection (here, FLOPROS as a placeholder, but that could be relaced by a domestic source, for instance DOST)\n",
    "The output may be a bit tricky.\n",
    "Note that the missing data report can be a bit tricky. Here some provinces are missing an exposure (fa=0), and that results in the vulnerability and vulnerability bias missing (as one divides by fa when computng them)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
